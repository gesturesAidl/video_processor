{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"main_tune_ASHA+HYPEROPT.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"xyZ5z0XEx-a7"},"source":["# INSTALL MODULES\n","%pip install pickle5\n","\n","# For ray.Tune\n","%pip install ray torch torchvision      # Preferred install command when running PyTorch\n","#%pip install -U ray                    # Alternate installation for Tune\n","\n","# For Tensorboard\n","%pip install tensorboardX\n","\n","# IMPORTS\n","import torch\n","from torch.utils.data import DataLoader\n","import torch.optim as optim\n","import numpy as np\n","import os\n","import pickle5 as pickle\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","from ray import tune\n","from ray.tune.schedulers import ASHAScheduler           # To use ASHA Scheduler (it's recommended using this over the standard HyperBand scheduler)\n","from hyperopt import hp\n","from ray.tune.suggest.hyperopt import HyperOptSearch\n","import tensorboardX\n","\n","# Load the TensorBoard notebook extension\n","%load_ext tensorboard\n","\n","# Mount Google Drive in Colab\n","#from google.colab import files\n","from google.colab import drive\n","drive.mount(\"/content/gdrive\")\n","\n","# Import modules in Colab from another notebooks (works!! :)) )\n","%run '/content/gdrive/MyDrive/Colab Notebooks/training/dataset.ipynb'\n","%run '/content/gdrive/MyDrive/Colab Notebooks/training/models.ipynb'\n","\n","if not torch.cuda.is_available():\n","    raise RuntimeError(\"You should enable GPU runtime.\")\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DkV1MMvJ8K8A"},"source":["# To speed up training, it's better to copy dataset from Drive to a Colab folder\n","\n","# choose a local (colab) directory to store the data.\n","local_dataset_path = os.path.expanduser('/content/data')\n","try:\n","  os.makedirs(local_dataset_path)\n","except: pass\n","\n","dataset_path = '/content/gdrive/MyDrive/jester_dataset/'\n","\n","!cp -avr \"{dataset_path}\" \"{local_dataset_path}\"\n","\n","# Make sure it's there\n","!ls -lha /content/gdrive/MyDrive/jester_dataset/features"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rxBa45SoyJmh"},"source":["device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","\n","# Retrieves features from i3d_resnet50_v1_kinetics400 Gluon pre-trained model for our 9 classes' Jester dataset videos\n","with open('/content/data/jester_dataset/features/features_RGB.pickle', 'rb') as handle:\n","    features_dict = pickle.load(handle)        # dict as {n_video:features, ...}  features: (1,2048)\n","    \n","csv_dir = '/content/data/jester_dataset/csv/'  # if in Google Cloud --> csv_dir = '/mnt/disks/disk-1/jester_dataset/dataset/csvs/' \n","train_csv = csv_dir + 'train.csv' \n","val_csv = csv_dir + 'validation.csv'\n","labels = csv_dir + 'labels.csv'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fxAIWu2XyY6W"},"source":["def train_epoch(model, train_loader, optimizer, criterion, epoch):  \n","  model.train()\n","  accs, losses = [], []\n","\n","  for features, labels in train_loader:\n","    optimizer.zero_grad()\n","    features, labels = features.to(device), labels.to(device)\n","    output = model(features)\n","    loss = criterion(output, labels)\n","    loss.backward()\n","    optimizer.step()\n","    accs.append(accuracy(labels, output))\n","    losses.append(loss.item())\n","\n","  return np.mean(losses), np.mean(accs)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hJ16o7Ru0mpZ"},"source":["def eval_epoch(model, val_loader, criterion, epoch): \n","  with torch.no_grad():\n","    model.eval()\n","    accs, losses = [], []\n","\n","    for features, labels in val_loader:\n","      features, labels = features.to(device), labels.to(device)\n","      output = model(features)\n","      loss = criterion(output, labels)\n","      accs.append(accuracy(labels, output))\n","      losses.append(loss.item())\n","\n","    return np.mean(losses), np.mean(accs)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KqehVqBp0ivg"},"source":["def accuracy(labels, outputs):\n","    preds = outputs.argmax(-1)\n","    acc = (preds == labels.view_as(preds)).cpu().float().detach().numpy().mean()\n","    return acc"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kI7nBvH21bc3"},"source":["def train_model(config):\n","  # DATASETS\n","  train_dataset = JesterDatasetOneStream(features_dict, train_csv, labels)\n","  validation_dataset = JesterDatasetOneStream(features_dict, val_csv, labels)\n","\n","  train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n","  validation_loader = DataLoader(validation_dataset, batch_size=config['batch_size'], shuffle=False)\n","\n","  # MODEL\n","  model = ClassifierOneStream(hidden_sz=config['hidden_size'], dropout=config['dropout']).to(device)\n","  optimizer = optim.Adam(model.parameters(), lr = config['lr'])\n","\n","  counts = pd.read_csv(train_csv).Action.value_counts()       # Counts = list of video counts for each class\n","  weights = torch.tensor([max(counts)/x for x in counts])     # calculates weights for all classes in training dataset (weight regularization)\n","  criterion = nn.CrossEntropyLoss(weight=weights).to(device)  # assigns weight to each of the classes. This is particularly useful when you have an unbalanced training set\n","  \n","  train_losses = []\n","  val_losses = []\n","  train_accs = []\n","  val_accs = []\n","  \n","  for epoch in range(config['epochs']+1):\n","    \n","    loss, acc = train_epoch(model, train_loader, optimizer, criterion, epoch)\n","    train_losses.append(loss)\n","    train_accs.append(acc)\n","    print(f\"Train Epoch {epoch} loss={loss:.2f} acc={acc:.2f}\")\n","        \n","    loss, acc = eval_epoch(model, validation_loader, criterion, epoch)\n","    val_losses.append(loss)\n","    val_accs.append(acc)\n","    print(f\"Eval Epoch {epoch} loss={loss:.2f} acc={acc:.2f}\")\n","    tune.report(tune_loss=loss, accuracy=acc)       # send report to Tune\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Sou9J7sp3FJD"},"source":["if __name__ == \"__main__\":\n","\n","    config = {\n","            \"lr\": hp.loguniform(\"lr\", np.log(1e-4), np.log(1e-2)),\n","            \"batch_size\": hp.choice(\"batch_size\", [8, 16, 32, 64, 128, 256]),\n","            \"hidden_size\": hp.choice(\"hidden_size\", [128, 256, 512, 1024, 2048]),      # 128, 256, 512, 1024 or 2048\n","            \"dropout\": 0.5,\n","            \"epochs\": 20,\n","    }\n","\n","    asha_scheduler = ASHAScheduler(             # For using ASHA (AsyncHyperBandScheduler) (recommended over Hyperband optimization)\n","            metric=\"tune_loss\",\n","            mode=\"min\",\n","            #time_attr='epoch',\n","            #max_t=config[\"epochs\"],\n","            grace_period=1,\n","            #reduction_factor=2,\n","    )\n","\n","    current_best_params = [{\n","            \"lr\": 0.00023,\n","            \"batch_size\": 64,\n","            \"hidden_size\": 1024,\n","            \"dropout\": 0.5,\n","            \"epochs\": 20,\n","    }]\n","\n","    hyperopt_search = HyperOptSearch(config, metric=\"tune_loss\", mode=\"min\", points_to_evaluate=current_best_params)\n","\n","    analysis = tune.run(                        # run hyperparameter tuning trials with Tune\n","        train_model,\n","        num_samples=50,\n","        #config=config,\n","        resources_per_trial={\"gpu\": 1},\n","        scheduler=asha_scheduler,\n","        search_alg=hyperopt_search,\n","        name=\"tune_RGB\"\n","        )     \n","\n","    print(\"\\nHyperparameter tuning finished\")\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3Zn_u_zNagFf"},"source":["# Obtain a trial dataframe from all run trials of this `tune.run` call.\n","dfs = analysis.trial_dataframes\n","\n","# Plot by epoch\n","ax = None  # This plots everything on the same plot\n","for d in dfs.values():\n","  ax = d.tune_loss.plot(ax=ax, legend=False)\n","ax.set_xlabel(\"Epochs\")\n","ax.set_ylabel(\"Validation loss\")\n","\n","# Get a dataframe for the last reported results of all of the trials\n","df_1 = analysis.results_df\n","\n","# Get a dataframe for the max accuracy seen for each trial\n","df_2 = analysis.dataframe(metric=\"tune_loss\", mode=\"min\")\n","\n","# Get a dict mapping {trial logdir -> dataframes} for all trials in the experiment.\n","all_dataframes = analysis.trial_dataframes\n","\n","# Get a list of trials\n","trials = analysis.trials"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"09rnnFgaOvhY"},"source":["%tensorboard --logdir ~/ray_results"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m_xCmCIIQqGW"},"source":["'''\n","import seaborn as sns\n","validation_dataset = JesterDataset(features_dict,val_csv,labels)\n","\n"," \n","validation_loader = DataLoader(validation_dataset, batch_size=config['batch_size'], shuffle=False)\n","nb_classes = 9\n","confusion_matrix = np.zeros((nb_classes, nb_classes))\n","trained_model = trained_model.to(device)\n","with torch.no_grad():\n","    for i, (inputs, classes) in enumerate(validation_loader):\n","        inputs = inputs.to(device)\n","        classes = classes.to(device)\n","        outputs = trained_model(inputs)\n","        _, preds = torch.max(outputs, 1)\n","        for t, p in zip(classes.view(-1), preds.view(-1)):\n","                confusion_matrix[t.long(), p.long()] += 1\n","\n","plt.figure(figsize=(12,7))\n","\n","l = pd.read_csv(labels)\n","class_names = list(l['Actions'])\n","l['Accuracy'] = np.diag(confusion_matrix)/confusion_matrix.sum(1)\n","df_cm = pd.DataFrame(confusion_matrix, index=class_names, columns=class_names).astype(int)\n","heatmap = sns.heatmap(df_cm, annot=True, fmt=\"d\")\n","\n","heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right',fontsize=12)\n","heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right',fontsize=12)\n","plt.ylabel('True label')\n","plt.xlabel('Predicted label')\n","'''"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"asLVEsQwX0B9"},"source":["#l"],"execution_count":null,"outputs":[]}]}